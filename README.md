## Social Media Generator (AI-Powered) üìù‚ö°

One-click social copy for marketers, founders, and creators.
Generate platform-optimized captions or posts for Twitter/X, Instagram, LinkedIn, TikTok, and Facebook.
Default provider: Ollama (local & free). Optional provider: OpenAI (paid).


## Overview

Social Media Generator is a Python + Streamlit web app that turns a plain-language product or campaign description into seven diverse, platform-optimized items:
- Captions (short, hooky, emoji-rich)
- Posts (long-form for LinkedIn/Facebook/Instagram, 400+ words where applicable)

It auto-adjusts tone, structure, and length based on your selections, ensures platform-specific constraints (e.g., Twitter/X 280-char cap), and includes copy-to-clipboard buttons for each item.

Who it‚Äôs for

- Marketers and social media managers shipping copy daily
- Small business owners who want a quick, on-brand caption bank
- Creators and teams needing consistent ideas with a polished format

## Features

- Platform support: Twitter/X, Instagram, LinkedIn, TikTok, Facebook
- Two providers:
    - Ollama (local & free) ‚Äî default; list installed models, pull new models in-app
    - OpenAI (paid) ‚Äî toggle on; model picker with safe fallbacks (e.g., gpt-4o-mini)
- Content types: Caption (concise) or Post (structured long-form, 400+ words on supported platforms)
- Word target control: Auto-sets by platform & type; app enforces 80‚Äì100% of your chosen target with a refine pass (non-Twitter)
- JSON-shaped outputs: Exactly 7 items for easy parsing/automation
- Style baked in: 2‚Äì3 emojis per item + CTA ending (e.g., ‚ÄúShop now!‚Äù, ‚ÄúRequest a demo!‚Äù)
- Robust UX: Copy buttons per item, warnings for truncation/trim/refine, rate-limit retries

## Tech Stack

| Component         | Version | Purpose                                         |
| ----------------- | ------: | ----------------------------------------------- |
| Python            |   3.10+ | Language/runtime                                |
| Streamlit         | 1.31.0+ | Web UI                                          |
| OpenAI Python SDK | 1.40.0+ | Chat Completions for GPT models                 |
| Requests          | 2.31.0+ | HTTP client (Ollama REST)                       |
| Ollama (binary)   |  latest | Local LLMs (server at `http://localhost:11434`) |
| Git               |  latest | Version control & CI/CD (optional)              |

## Architecture
High-Level Diagram

To Be Added

## Components
- Prompt Template
F-string with the user‚Äôs description, platform, tone, content type, and word target. Includes few-shot examples, sets temperature (0.85 default) and requires JSON output (7 items).

- Provider Layer
    - Ollama: /api/chat POST with model name; supports listing local models and pulling new ones
    - OpenAI: chat.completions.create(...) with the selected model (e.g., gpt-4o-mini)

- Post-Processing
    - Parse/repair JSON
    - Enforce Twitter/X 280-char rule
    - Enforce 80‚Äì100% word window (trim if over; refine once if under, except on Twitter)

- UI Layer
    - Sidebar inputs (provider toggle, key/model selection, description, platform, tone, content type, word target)
    - Results with expanders, word + char counts, copy buttons, and warnings

## Installation & Setup

Works on Windows/macOS/Linux. We recommend Conda for a clean Python 3.10 environment.

1) Clone the repo
```
git clone https://github.com/malmehayla/social_media_generator.git
cd social_media_generator
```

2) Create & activate Conda environment

Option A ‚Äî environment.yml (recommended)
```
conda env create -f environment.yml
conda activate captiongen
```

Option B ‚Äî manual
```
conda create -n captiongen python=3.10 -y
conda activate captiongen
pip install --upgrade pip
pip install -r requirements.txt
```

3) Install & run Ollama (default provider)

Windows: winget install Ollama.Ollama
macOS: brew install ollama
Linux: see https://ollama.com

Pull at least one chat-friendly model (you can also pull inside the app)
```
ollama pull llama3.2:3b
# other good options:
#   qwen2.5:3b-instruct
#   llama3.1:8b
```
Test server:
```
curl http://localhost:11434/api/version
```